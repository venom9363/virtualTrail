{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.protobuf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clothing image (e.g., a transparent PNG of a t-shirt)\n",
    "clothing_img = cv2.imread(\"tshirt.png\", -1)  # Ensure it's a transparent image (RGBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "def overlay_clothing(frame, clothing_img, landmarks):\n",
    "    # Get coordinates of key body landmarks\n",
    "    shoulder_left = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    shoulder_right = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "    hip_left = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "\n",
    "    # Calculate width and height of the t-shirt based on the distance between shoulders and torso height\n",
    "    width = int(abs(shoulder_right.x - shoulder_left.x) * frame.shape[1])\n",
    "    height = int(abs(hip_left.y - shoulder_left.y) * frame.shape[0])\n",
    "\n",
    "    # Ensure valid width and height\n",
    "    if width <= 0 or height <= 0:\n",
    "        return frame\n",
    "\n",
    "    # Resize the t-shirt image to match calculated dimensions\n",
    "    clothing_resized = cv2.resize(clothing_img, (width, height))\n",
    "\n",
    "    # Calculate the center point between the shoulders\n",
    "    center_x = int((shoulder_left.x + shoulder_right.x) / 2 * frame.shape[1])\n",
    "\n",
    "    # Calculate top-left corner of the t-shirt for positioning\n",
    "    top_left_x = center_x - width // 2\n",
    "    top_left_y = int(shoulder_left.y * frame.shape[0])\n",
    "\n",
    "    # Ensure the t-shirt stays within the frame boundaries\n",
    "    if top_left_x + width > frame.shape[1]:\n",
    "        width = frame.shape[1] - top_left_x\n",
    "    if top_left_y + height > frame.shape[0]:\n",
    "        height = frame.shape[0] - top_left_y\n",
    "\n",
    "    # Resize the t-shirt again if necessary\n",
    "    clothing_resized = cv2.resize(clothing_resized, (width, height))\n",
    "\n",
    "    # Extract the alpha channel for transparency (assuming PNG with transparency)\n",
    "    alpha_s = clothing_resized[:, :, 3] / 255.0\n",
    "    alpha_l = 1.0 - alpha_s\n",
    "\n",
    "    # Overlay the t-shirt onto the frame\n",
    "    for c in range(0, 3):  # Loop over color channels\n",
    "        frame[top_left_y:top_left_y + height, top_left_x:top_left_x + width, c] = (\n",
    "            alpha_s * clothing_resized[:, :, c] +\n",
    "            alpha_l * frame[top_left_y:top_left_y + height, top_left_x:top_left_x + width, c]\n",
    "        )\n",
    "\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def overlay_clothing(frame, clothing_img, landmarks):\\n    # Get the size of the clothing image\\n    clothing_h, clothing_w, _ = clothing_img.shape\\n    \\n    # Get body landmarks (shoulder and hip points)\\n    shoulder_left = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\\n    shoulder_right = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\\n    hip_left = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\\n    hip_right = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\\n\\n    # Calculate the region where the clothing should be placed\\n    width = int(abs(shoulder_right.x - shoulder_left.x) * frame.shape[1])\\n    height = int(abs(hip_left.y - shoulder_left.y) * frame.shape[0])\\n\\n    # Ensure width and height are within bounds\\n    width = min(width, frame.shape[1])\\n    height = min(height, frame.shape[0])\\n\\n    # Resize clothing image to fit on the person's body\\n    clothing_resized = cv2.resize(clothing_img, (width, height))\\n\\n    # Find the coordinates for positioning the clothing on the body\\n    top_left_x = int(shoulder_left.x * frame.shape[1])\\n    top_left_y = int(shoulder_left.y * frame.shape[0])\\n\\n    # Ensure that the placement is within the frame boundaries\\n    if top_left_x + width > frame.shape[1]:\\n        width = frame.shape[1] - top_left_x\\n    if top_left_y + height > frame.shape[0]:\\n        height = frame.shape[0] - top_left_y\\n\\n    # Resize clothing image again to match the updated width and height if needed\\n    clothing_resized = cv2.resize(clothing_resized, (width, height))\\n\\n    # Get the alpha channel of the clothing image for transparency\\n    alpha_s = clothing_resized[:, :, 3] / 255.0  # Alpha channel for clothing\\n    alpha_l = 1.0 - alpha_s  # Inverted alpha for background blending\\n\\n    # Perform overlay only where dimensions match\\n    for c in range(0, 3):  # Loop over color channels\\n        frame[top_left_y:top_left_y + height, top_left_x:top_left_x + width, c] = (\\n            alpha_s * clothing_resized[:, :, c] +\\n            alpha_l * frame[top_left_y:top_left_y + height, top_left_x:top_left_x + width, c]\\n        )\\n\\n    return frame\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def overlay_clothing(frame, clothing_img, landmarks):\n",
    "    # Get the size of the clothing image\n",
    "    clothing_h, clothing_w, _ = clothing_img.shape\n",
    "    \n",
    "    # Get body landmarks (shoulder and hip points)\n",
    "    shoulder_left = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    shoulder_right = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "    hip_left = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "    hip_right = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "\n",
    "    # Calculate the region where the clothing should be placed\n",
    "    width = int(abs(shoulder_right.x - shoulder_left.x) * frame.shape[1])\n",
    "    height = int(abs(hip_left.y - shoulder_left.y) * frame.shape[0])\n",
    "\n",
    "    # Ensure width and height are within bounds\n",
    "    width = min(width, frame.shape[1])\n",
    "    height = min(height, frame.shape[0])\n",
    "\n",
    "    # Resize clothing image to fit on the person's body\n",
    "    clothing_resized = cv2.resize(clothing_img, (width, height))\n",
    "\n",
    "    # Find the coordinates for positioning the clothing on the body\n",
    "    top_left_x = int(shoulder_left.x * frame.shape[1])\n",
    "    top_left_y = int(shoulder_left.y * frame.shape[0])\n",
    "\n",
    "    # Ensure that the placement is within the frame boundaries\n",
    "    if top_left_x + width > frame.shape[1]:\n",
    "        width = frame.shape[1] - top_left_x\n",
    "    if top_left_y + height > frame.shape[0]:\n",
    "        height = frame.shape[0] - top_left_y\n",
    "\n",
    "    # Resize clothing image again to match the updated width and height if needed\n",
    "    clothing_resized = cv2.resize(clothing_resized, (width, height))\n",
    "\n",
    "    # Get the alpha channel of the clothing image for transparency\n",
    "    alpha_s = clothing_resized[:, :, 3] / 255.0  # Alpha channel for clothing\n",
    "    alpha_l = 1.0 - alpha_s  # Inverted alpha for background blending\n",
    "\n",
    "    # Perform overlay only where dimensions match\n",
    "    for c in range(0, 3):  # Loop over color channels\n",
    "        frame[top_left_y:top_left_y + height, top_left_x:top_left_x + width, c] = (\n",
    "            alpha_s * clothing_resized[:, :, c] +\n",
    "            alpha_l * frame[top_left_y:top_left_y + height, top_left_x:top_left_x + width, c]\n",
    "        )\n",
    "\n",
    "    return frame\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Process the image to detect the pose\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mpose_landmarks:\n\u001b[0;32m     16\u001b[0m     mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(frame, result\u001b[38;5;241m.\u001b[39mpose_landmarks, mp_pose\u001b[38;5;241m.\u001b[39mPOSE_CONNECTIONS)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB as required by MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the image to detect the pose\n",
    "    result = pose.process(frame_rgb)\n",
    "    \n",
    "    if result.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        landmarks = result.pose_landmarks.landmark\n",
    "\n",
    "        # Overlay clothing on detected landmarks\n",
    "        frame = overlay_clothing(frame, clothing_img, landmarks)\n",
    "    \n",
    "    cv2.imshow('Virtual Dressing Room', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#njnjnjnjnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
